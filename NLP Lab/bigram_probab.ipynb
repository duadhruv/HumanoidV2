{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(Bigram -  Probabilities)Jatin_Rishi_18BCS060_NLP_LAB_Prog_3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN7UJb+fOyu0YtjKnPGxZrC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6ohcgRCzzR3","executionInfo":{"status":"ok","timestamp":1644253094748,"user_tz":-330,"elapsed":554,"user":{"displayName":"Jatin Rishi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr_Ep47EZKZKoBgbvuxyV_IfNoWL7BdHee3eZRvA=s64","userId":"16149200814378969705"}},"outputId":"3028022a-33cc-4ff7-f1bd-d0d85cfa4dfe"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","\n","\n","INPUT CORPUS :- \n","### abstract ###\n","The problem of joint universal source coding and modeling, addressed by Rissanen in the context of lossless codes, is generalized to fixed-rate lossy coding of continuous-alphabet memoryless sources\n","We show that, for bounded distortion measures, any compactly parametrized family of  iid \n","real vector sources with absolutely continuous marginals (satisfying appropriate smoothness and Vapnik--Chervonenkis learnability conditions) admits a joint scheme for universal lossy block coding and parameter estimation, and give nonasymptotic estimates of convergence rates for distortion redundancies and variational distances between the active source and the estimated source\n","We also present explicit examples of parametric sources admitting such joint universal compression and modeling schemes\n","### introduction ###\n","In universal data compression, a single code achieves asymptotically optimal performance on all sources within a given family\n","Intuition suggests that a good universal coder should acquire an accurate model of the source statistics from a sufficiently long data sequence and incorporate this knowledge in its operation\n","For lossless codes, this intuition has been made rigorous by Rissanen  CITATION\n","Under his scheme, the data are encoded in a  two-stage  set-up, in which the binary representation of each source block consists of two parts: (1) a suitably quantized maximum-likelihood estimate of the source parameters, and (2) lossless encoding of the data matched to the acquired model; the redundancy of the resulting code converges to zero as  SYMBOL , where  SYMBOL  is the block length\n","In this paper, we extend Rissanen's idea to  lossy  block coding (vector quantization) of  iid \n","sources with values in  SYMBOL  for some finite  SYMBOL\n","Specifically, let  SYMBOL  be an  iid \n","source with the marginal distribution of  SYMBOL  belonging to some indexed class  SYMBOL  of absolutely continuous distributions on  SYMBOL , where  SYMBOL  is a bounded subset of  SYMBOL  for some  SYMBOL\n","For bounded distortion measures, our main result, Theorem~, states that if the class  SYMBOL  satisfies certain smoothness and learnability conditions, then there exists a sequence of finite-memory lossy block codes that achieves asymptotically optimal compression of each source in the class and permits asymptotically exact identification of the active source with respect to the  variational distance , defined as  SYMBOL , where the supremum is over all Borel subsets of  SYMBOL\n","The overhead rate and the distortion redundancy of the scheme converge to zero as  SYMBOL  and  SYMBOL , respectively, where  SYMBOL  is the block length, while the active source can be identified up to a variational ball of radius  SYMBOL  eventually almost surely\n","We also describe an extension of our scheme to unbounded distortion measures satisfying a certain moment condition, and present two examples of parametric families satisfying the regularity conditions of Theorem~\n","While most existing schemes for universal lossy coding rely on  implicit  identification of the active source (e g , through topological covering arguments  CITATION , Glivenko--Cantelli uniform laws of large numbers  CITATION , or nearest-neighbor code clustering  CITATION ), our code builds an  explicit model  of the mechanism responsible for generating the data and then selects an appropriate code for the data on the basis of the model\n","This ability to simultaneously model and compress the data may prove useful in such applications as  media forensics   CITATION , where the parameter  SYMBOL  could represent evidence of tampering, and the aim is to compress the data in such a way that the evidence can be later extracted with high fidelity from the compressed version\n","Another key feature of our approach is the use of Vapnik--Chervonenkis theory  CITATION  in order to connect universal encodability of a class of sources to the combinatorial ``richness\" of a certain collection of decision regions associated with the sources\n","In a way, Vapnik--Chervonenkis estimates can be thought of as an (imperfect) analogue of the combinatorial method of types for finite alphabets  CITATION\n","\n"]}],"source":["import string\n","import random\n","import nltk\n","nltk.download('stopwords')\n","from nltk import FreqDist\n","from nltk.corpus import stopwords\n","from nltk.util import ngrams\n","from collections import *\n","from nltk.corpus import reuters\n","\n","\n","corpus = open(\"input_corpus.txt\").read()\n","print(\"\\n\\nINPUT CORPUS :- \")\n","print(corpus)\n"]},{"cell_type":"code","source":["def  setNLTK():\n","  sents = corpus.split('\\n')\n","  stop_words = set(stopwords.words('english'))\n","  string.punctuation = string.punctuation +'\"'+'\"'+'-'+'''+'''+'â€”'\n","  removal_list = list(stop_words) + list(string.punctuation)+ ['lt','rt']\n","  return (sents,stop_words,removal_list)\n","\n","(sents,stop_words,removal_list) = setNLTK()\n","\n","def remove_redundant_stopwords(x):    \n","    y = []\n","    for pair in x:\n","        count = 0\n","        for word in pair:\n","            if word in removal_list:\n","                count = count or 0\n","            else:\n","                count = count or 1\n","        if (count==1):\n","            y.append(pair)\n","    return (y)\n","\n","def get_randomised_word(d,counter):\n","    try:\n","        return random.choice(d[counter])\n","    except:\n","        return \".\"\n","\n","unigram = []\n","bigram=[]\n","trigram=[]\n","quadrigram=[]\n","tokenized_text=[]\n","for sentence in sents:\n","    sentence = list(map(lambda x:x.lower(),sentence.split()))\n","    for word in sentence:\n","        if word == '.':\n","            sentence.remove(word)\n","        else:\n","            unigram.append(word)\n","    \n","    tokenized_text.append(sentence)\n","    bigram.extend(list(ngrams(sentence, 2,pad_left=True, pad_right=True)))\n","    quadrigram.extend(list(ngrams(sentence, 4, pad_left=True, pad_right=True)))\n","\n","\n","unigram = remove_redundant_stopwords(unigram)\n","bigram = remove_redundant_stopwords(bigram)\n","quadrigram = remove_redundant_stopwords(quadrigram)\n"],"metadata":{"id":"A-K62NN26mSv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def bigram_probablities_prediction(bigram):\n","  bigram_frequency_dict = FreqDist(bigram)\n","  d = {}\n","  for a1, a2 in bigram_frequency_dict:\n","      if a1==None or a2==None :\n","        continue\n","      try:\n","            d[a1].extend([a2]*bigram_frequency_dict[a1,a2])\n","      except:\n","            d[a1] = [a2]*bigram_frequency_dict[a1,a2]\n","  \n","  print(\"Word Choosen for Bigram\")\n","  s = ''\n","  prefix = \"identification\"\n","  print(prefix)\n","  s = prefix\n","  for i in range(5):\n","      suffix = get_randomised_word(d,prefix)\n","      s=s+' '+suffix\n","      print(f\"Prediction for {i+1}th word in the sentence : \")\n","      print(s)\n","      print(\"------------------------------------------------------\")\n","      prefix = suffix\n","\n","bigram_probablities_prediction(bigram)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnkMh-sc9RDN","executionInfo":{"status":"ok","timestamp":1644253094751,"user_tz":-330,"elapsed":25,"user":{"displayName":"Jatin Rishi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr_Ep47EZKZKoBgbvuxyV_IfNoWL7BdHee3eZRvA=s64","userId":"16149200814378969705"}},"outputId":"a4888d7d-4506-4436-c9d9-c15eadd91b22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Choosen for Bigram\n","identification\n","Prediction for 1th word in the sentence : \n","identification of\n","------------------------------------------------------\n","Prediction for 2th word in the sentence : \n","identification of finite-memory\n","------------------------------------------------------\n","Prediction for 3th word in the sentence : \n","identification of finite-memory lossy\n","------------------------------------------------------\n","Prediction for 4th word in the sentence : \n","identification of finite-memory lossy coding\n","------------------------------------------------------\n","Prediction for 5th word in the sentence : \n","identification of finite-memory lossy coding of\n","------------------------------------------------------\n"]}]},{"cell_type":"code","source":["def quadrigram_probabilities_prediction(quadrigram):\n","  quadrigram_frequency_dict = FreqDist(quadrigram)\n","  d = {}\n","  for a1, a2, a3, a4 in quadrigram_frequency_dict:\n","      if(a1 == None or a2== None or a3== None or a4== None):\n","        continue\n","      try:\n","          d[a1,a2,a3].extend([a4]*quadrigram_frequency_dict[a1,a2,a3,a4])\n","      except:\n","          d[a1,a2,a3] = [a4]*quadrigram_frequency_dict[a1,a2,a3,a4]\n","  \n","  print(\"Word choosen for quadrigram\")\n","  s=''\n","  prefix = (\"while\" ,\"most\" ,\"existing\")\n","  print(\" \".join(prefix))\n","  s = \" \".join(prefix)\n","  for i in range(5):\n","      suffix = get_randomised_word(d,prefix)\n","      s=s+' '+suffix\n","      print(f\"Prediction for {i+1}th word in the sentence : \")\n","      print(s)\n","      print(\"------------------------------------------------------\")\n","      prefix = prefix[1], prefix[2], suffix\n","\n","quadrigram_probabilities_prediction(quadrigram)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g4Pprmid_D89","executionInfo":{"status":"ok","timestamp":1644253094753,"user_tz":-330,"elapsed":19,"user":{"displayName":"Jatin Rishi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghr_Ep47EZKZKoBgbvuxyV_IfNoWL7BdHee3eZRvA=s64","userId":"16149200814378969705"}},"outputId":"e7ac3052-423f-4cce-829c-e7e3475568cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word choosen for quadrigram\n","while most existing\n","Prediction for 1th word in the sentence : \n","while most existing schemes\n","------------------------------------------------------\n","Prediction for 2th word in the sentence : \n","while most existing schemes for\n","------------------------------------------------------\n","Prediction for 3th word in the sentence : \n","while most existing schemes for universal\n","------------------------------------------------------\n","Prediction for 4th word in the sentence : \n","while most existing schemes for universal lossy\n","------------------------------------------------------\n","Prediction for 5th word in the sentence : \n","while most existing schemes for universal lossy block\n","------------------------------------------------------\n"]}]}]}